{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Step:  0 \t Loss:  1.00032306\n",
      "W1:  [[0.769314051 0.374498963]\n",
      " [-0.0571876429 0.00471873302]] \t W2:  [[0.342847496]\n",
      " [-0.272891641]]\n",
      "Step:  1000 \t Loss:  0.693004\n",
      "W1:  [[0.762940407 0.357756436]\n",
      " [-0.153855056 -0.0394066907]] \t W2:  [[0.155675873]\n",
      " [-0.897824526]]\n",
      "Step:  2000 \t Loss:  0.691491425\n",
      "W1:  [[0.841490209 0.349691421]\n",
      " [-0.398999482 -0.108898751]] \t W2:  [[0.407773465]\n",
      " [-0.912501633]]\n",
      "Step:  3000 \t Loss:  0.675787866\n",
      "W1:  [[1.2871542 0.505471]\n",
      " [-1.15666 -0.272968143]] \t W2:  [[1.11888373]\n",
      " [-1.01361811]]\n",
      "Step:  4000 \t Loss:  0.489138663\n",
      "W1:  [[2.99547791 1.73985684]\n",
      " [-3.18689704 -1.43698788]] \t W2:  [[3.30448842]\n",
      " [-2.09208155]]\n",
      "Step:  5000 \t Loss:  0.14952606\n",
      "W1:  [[4.51025438 3.9628129]\n",
      " [-4.6665535 -3.66153097]] \t W2:  [[5.84639549]\n",
      " [-5.01734495]]\n",
      "Step:  6000 \t Loss:  0.0671663\n",
      "W1:  [[5.12131834 4.85792398]\n",
      " [-5.27038145 -4.56899118]] \t W2:  [[7.25984192]\n",
      " [-6.62003]]\n",
      "Step:  7000 \t Loss:  0.0414245278\n",
      "W1:  [[5.44596338 5.30720949]\n",
      " [-5.60027838 -5.02377176]] \t W2:  [[8.12431049]\n",
      " [-7.54617405]]\n",
      "Step:  8000 \t Loss:  0.0295518376\n",
      "W1:  [[5.65908146 5.59279203]\n",
      " [-5.81936216 -5.31261396]] \t W2:  [[8.73523235]\n",
      " [-8.18461227]]\n",
      "Step:  9000 \t Loss:  0.0228384044\n",
      "W1:  [[5.81528854 5.79779959]\n",
      " [-5.98082495 -5.51981544]] \t W2:  [[9.20489407]\n",
      " [-8.66908169]]\n",
      "Step:  10000 \t Loss:  0.0185553357\n",
      "W1:  [[5.9375267 5.95587158]\n",
      " [-6.10755396 -5.67947531]] \t W2:  [[9.58539486]\n",
      " [-9.05853939]]\n",
      "\n",
      "Hypothesis:  tf.Tensor(\n",
      "[[0.01798218]\n",
      " [0.9756787 ]\n",
      " [0.9839669 ]\n",
      " [0.01517421]], shape=(4, 1), dtype=float32) \n",
      "Correct:  tf.Tensor(\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]], shape=(4, 1), dtype=float32) \n",
      "Accuracy:  tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHypothesis:  [[ 0.01338224]\\n [ 0.98166382]\\n [ 0.98809403]\\n [ 0.01135806]]\\nCorrect:  [[ 0.]\\n [ 1.]\\n [ 1.]\\n [ 0.]]\\nAccuracy:  1.0\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 9 XOR-back_prop\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.find('2') == 0)\n",
    "tf.random.set_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.Variable(x_data)\n",
    "Y = tf.Variable(y_data)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([2]), name='bias1')\n",
    "l1 = lambda: tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1]), name='bias2')\n",
    "Y_pred = lambda: tf.sigmoid(tf.matmul(l1(), W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "@tf.function\n",
    "def cost():\n",
    "    return -tf.reduce_mean(Y * tf.math.log(Y_pred()) + (1 - Y) *\n",
    "                           tf.math.log(1 - Y_pred()))\n",
    "\n",
    "# Network\n",
    "#          p1     a1           l1     p2     a2           l2 (y_pred)\n",
    "# X -> (*) -> (+) -> (sigmoid) -> (*) -> (+) -> (sigmoid) -> (loss)\n",
    "#       ^      ^                   ^      ^\n",
    "#       |      |                   |      |\n",
    "#       W1     b1                  W2     b2\n",
    "\n",
    "# Loss derivative\n",
    "d_Y_pred = lambda: (Y_pred() - Y) / (Y_pred() * (1.0 - Y_pred()) + 1e-7)\n",
    "\n",
    "# Layer 2\n",
    "d_sigma2 = lambda: Y_pred() * (1 - Y_pred())\n",
    "d_a2 = lambda: d_Y_pred() * d_sigma2()\n",
    "d_p2 = d_a2\n",
    "d_b2 = d_a2\n",
    "d_W2 = lambda: tf.matmul(tf.transpose(l1()), d_p2())\n",
    "\n",
    "# Mean\n",
    "d_b2_mean = lambda: tf.reduce_mean(d_b2(), axis=[0])\n",
    "d_W2_mean = lambda: d_W2() / tf.cast(tf.shape(l1())[0], dtype=tf.float32)\n",
    "\n",
    "# Layer 1\n",
    "d_l1 = lambda: tf.matmul(d_p2(), tf.transpose(W2))\n",
    "d_sigma1 = lambda: l1() * (1 - l1())\n",
    "d_a1 = lambda: d_l1() * d_sigma1()\n",
    "d_b1 = d_a1\n",
    "d_p1 = d_a1\n",
    "d_W1 = lambda: tf.matmul(tf.transpose(X), d_a1())\n",
    "\n",
    "# Mean\n",
    "d_W1_mean = lambda: d_W1() / tf.cast(tf.shape(X)[0], dtype=tf.float32)\n",
    "d_b1_mean = lambda: tf.reduce_mean(d_b1(), axis=[0])\n",
    "\n",
    "# Weight update\n",
    "step = [\n",
    "  lambda: W2.assign(W2 - learning_rate * d_W2_mean()),\n",
    "  lambda: b2.assign(b2 - learning_rate * d_b2_mean()),\n",
    "  lambda: W1.assign(W1 - learning_rate * d_W1_mean()),\n",
    "  lambda: b1.assign(b1 - learning_rate * d_b1_mean())\n",
    "]\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = lambda: tf.cast(Y_pred() > 0.5, dtype=tf.float32)\n",
    "accuracy = lambda: tf.reduce_mean(tf.cast(tf.equal(predicted(), Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "# @tf.function\n",
    "def run():\n",
    "    for i in range(10001):\n",
    "        step[0]()\n",
    "        step[1]()\n",
    "        step[2]()\n",
    "        step[3]()\n",
    "        cost_val = cost()\n",
    "        if i % 1000 == 0:\n",
    "            tf.print(\n",
    "                \"Step: \", i, \"\\t Loss: \", cost_val\n",
    "            )\n",
    "            tf.print(\n",
    "                \"W1: \", W1, \"\\t W2: \", W2\n",
    "            )\n",
    "            \n",
    "print(\"Running...\")\n",
    "run()\n",
    "\n",
    "# Accuracy report\n",
    "h, c, a = Y_pred(), predicted(), accuracy()\n",
    "print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "\n",
    "'''\n",
    "Hypothesis:  [[ 0.01338224]\n",
    " [ 0.98166382]\n",
    " [ 0.98809403]\n",
    " [ 0.01135806]]\n",
    "Correct:  [[ 0.]\n",
    " [ 1.]\n",
    " [ 1.]\n",
    " [ 0.]]\n",
    "Accuracy:  1.0\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
