{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 cost:  5.68203449\n",
      "Step:  2000 cost:  0.454490781\n",
      "Step:  4000 cost:  0.373116344\n",
      "Step:  6000 cost:  0.32182318\n",
      "Step:  8000 cost:  0.283468723\n",
      "Step:  10000 cost:  0.253114849\n",
      "Step:  12000 cost:  0.228399932\n",
      "Step:  14000 cost:  0.207887754\n",
      "Step:  16000 cost:  0.190606862\n",
      "Step:  18000 cost:  0.175865769\n",
      "Step:  20000 cost:  0.163155675\n",
      "--------------\n",
      "[[0.023961924 0.976028323 9.76851788e-06]] [1]\n",
      "--------------\n",
      "[[0.713546872 0.257215947 0.0292372461]] [0]\n",
      "--------------\n",
      "[[0.000263624388 0.172967941 0.826768398]] [2]\n",
      "--------------\n",
      "[[0.440473229 0.559526742 9.97446836e-09]\n",
      " [0.988881 0.0111168195 2.25072e-06]\n",
      " [0.000263624388 0.172967941 0.826768398]] [1 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 6.926112\\n200 0.6005015\\n400 0.47295815\\n600 0.37342924\\n800 0.28018373\\n1000 0.23280522\\n1200 0.21065344\\n1400 0.19229904\\n1600 0.17682323\\n1800 0.16359556\\n2000 0.15216158\\n-------------\\n[[1.3890490e-03 9.9860185e-01 9.0613084e-06]] [1]\\n-------------\\n[[0.9311919  0.06290216 0.00590591]] [0]\\n-------------\\n[[1.2732815e-08 3.3411323e-04 9.9966586e-01]] [2]\\n-------------\\n[[1.3890490e-03 9.9860185e-01 9.0613084e-06]\\n [9.3119192e-01 6.2902197e-02 5.9059085e-03]\\n [1.2732815e-08 3.3411323e-04 9.9966586e-01]] [1 0 2]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "from functools import partial as bind\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.find('2') == 0)\n",
    "\n",
    "tf.random.set_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = tf.Variable([[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]], dtype=tf.float32)\n",
    "y_data = tf.Variable([[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]], dtype=tf.float32)\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = lambda X: tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "@tf.function\n",
    "def cost(X, Y):\n",
    "    return tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(hypothesis(X)), axis=1))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Launch graph\n",
    "for step in range(20001):\n",
    "    # Minimize\n",
    "    optimizer.minimize(bind(cost, x_data, y_data), var_list=[W, b])\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "        # evaluate training accuracy\n",
    "        tf.print(\"Step: \", step, \"cost: \", bind(cost, x_data, y_data)())\n",
    "\n",
    "print('--------------')\n",
    "# Testing & One-hot encoding\n",
    "a = hypothesis(tf.Variable([[1, 11, 7, 9]], dtype=tf.float32))\n",
    "tf.print(a, tf.argmax(a, 1))\n",
    "\n",
    "print('--------------')\n",
    "b = hypothesis(tf.Variable([[1, 3, 4, 3]], dtype=tf.float32))\n",
    "tf.print(b, tf.argmax(b, 1))\n",
    "\n",
    "print('--------------')\n",
    "c = hypothesis(tf.Variable([[1, 1, 0, 1]], dtype=tf.float32))\n",
    "tf.print(c, tf.argmax(c, 1))\n",
    "\n",
    "print('--------------')\n",
    "all = hypothesis(tf.Variable([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]], dtype=tf.float32))\n",
    "tf.print(all, tf.argmax(all, 1))\n",
    "\n",
    "'''\n",
    "0 6.926112\n",
    "200 0.6005015\n",
    "400 0.47295815\n",
    "600 0.37342924\n",
    "800 0.28018373\n",
    "1000 0.23280522\n",
    "1200 0.21065344\n",
    "1400 0.19229904\n",
    "1600 0.17682323\n",
    "1800 0.16359556\n",
    "2000 0.15216158\n",
    "-------------\n",
    "[[1.3890490e-03 9.9860185e-01 9.0613084e-06]] [1]\n",
    "-------------\n",
    "[[0.9311919  0.06290216 0.00590591]] [0]\n",
    "-------------\n",
    "[[1.2732815e-08 3.3411323e-04 9.9966586e-01]] [2]\n",
    "-------------\n",
    "[[1.3890490e-03 9.9860185e-01 9.0613084e-06]\n",
    " [9.3119192e-01 6.2902197e-02 5.9059085e-03]\n",
    " [1.2732815e-08 3.3411323e-04 9.9966586e-01]] [1 0 2]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
